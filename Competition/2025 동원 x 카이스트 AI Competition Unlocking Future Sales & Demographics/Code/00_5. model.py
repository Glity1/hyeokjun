# train_lgbm.py
import os
import gc
import json
import math
import numpy as np
import pandas as pd
from datetime import datetime

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from lightgbm import LGBMRegressor, early_stopping, log_evaluation

# =========================
# 0) ê²½ë¡œ/ì„¤ì •
# =========================
FEATURE_DIR = "./_save/features"
TRAIN_FEAT  = os.path.join(FEATURE_DIR, "train_features.csv")
TEST_FEAT   = os.path.join(FEATURE_DIR, "test_features_2025H1.csv")
LAG_TABLE   = os.path.join(FEATURE_DIR, "best_lag_table.csv")

# íŒë§¤ëŸ‰(ì •ë‹µ) íŒŒì¼: ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.
#  - ì»¬ëŸ¼ ì˜ˆ: product_name,date,target   (dateëŠ” 'YYYY-MM-01')
TRAIN_TARGET_PATH = "./_data/dacon/dongwon/train_sales.csv"   # <- ì—¬ê¸°ì— ì‹¤ì œ ê²½ë¡œ ì§€ì •

SAMPLE_SUB_PATH   = "./_data/dacon/dongwon/sample_submission.csv"  # ëŒ€íšŒ ì œê³µ
PRODUCT_INFO_PATH = "./_data/dacon/dongwon/product_info.csv"       # (ì„ íƒ) ë§¤í•‘ì— ì‚¬ìš©

SAVE_DIR = "./_save/models"
os.makedirs(SAVE_DIR, exist_ok=True)

TARGET_COL = "target"  # íŒë§¤ëŸ‰(ë˜ëŠ” ëŒ€íšŒ íƒ€ê¹ƒ) ì»¬ëŸ¼ëª…
KEY_COLS   = ["product_name", "date"]

SEEDS = [42, 74, 99, 111, 333]
N_SPLITS = 5

# =========================
# 1) ë¡œë“œ & ë³‘í•© (A: íƒ€ê¹ƒ ì¡°ì¸)
# =========================
train_feat = pd.read_csv(TRAIN_FEAT, parse_dates=["date"])
test_feat  = pd.read_csv(TEST_FEAT,  parse_dates=["date"])

# íƒ€ê¹ƒ ë¡œë“œ
if not os.path.exists(TRAIN_TARGET_PATH):
    raise FileNotFoundError(
        f"[í•„ìˆ˜] íŒë§¤ëŸ‰ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {TRAIN_TARGET_PATH}\n"
        f"ìµœì†Œ ì»¬ëŸ¼ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {KEY_COLS + [TARGET_COL]}\n"
        f"ì˜ˆ: product_name,date,target  (date=YYYY-MM-01)"
    )
train_tgt = pd.read_csv(TRAIN_TARGET_PATH, parse_dates=["date"])

# ì¡°ì¸ í‚¤ ì •ê·œí™”
def normalize_dates(df):
    # ì¼ìë¥¼ ì›”ì´ˆë¡œ ì •ê·œí™”(ì˜ˆ: 2025-06-17 -> 2025-06-01)
    df = df.copy()
    df["date"] = df["date"].dt.to_period("M").dt.to_timestamp()
    return df

train_feat = normalize_dates(train_feat)
test_feat  = normalize_dates(test_feat)
train_tgt  = normalize_dates(train_tgt)

# ì¡°ì¸
train = pd.merge(train_feat, train_tgt[KEY_COLS+[TARGET_COL]], on=KEY_COLS, how="inner")
print(f"[INFO] í•™ìŠµ ë°ì´í„° í¬ê¸°: {train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_feat.shape}")

# =========================
# 2) í”¼ì²˜ ì„ íƒ (B: ê²€ì¦ì „ëµ ì „ì— ì»¬ëŸ¼ í™•ì •)
# =========================
# í›„ë³´ í”¼ì²˜ ìë™ ì„ íƒ
num_cols = []
for c in train.columns:
    if c in KEY_COLS + [TARGET_COL]:
        continue
    if train[c].dtype.kind in "biufc":   # ìˆ«ìí˜•
        num_cols.append(c)

# ì¶”ì²œ í¬í•¨: search/click ì›ì‹œê°’, *_mom, *_yoy, *_ma3/ma6, search_index_bestlag, month/sin/cos, missing_click_flag
# í•„ìš”ì‹œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¡œ ì¢íˆê¸°:
# whitelist = ["search_index","clicks","search_index_mom","search_index_yoy",
#              "clicks_mom","clicks_yoy","search_index_ma3","search_index_ma6",
#              "clicks_ma3","clicks_ma6","search_index_bestlag","month","sin_m","cos_m",
#              "missing_click_flag","best_lag"]
# num_cols = [c for c in num_cols if c in whitelist and c in train.columns]

print(f"[INFO] ì‚¬ìš© í”¼ì²˜ ìˆ˜: {len(num_cols)}")
print("[INFO] ìƒ˜í”Œ í”¼ì²˜:", num_cols[:20])

# =========================
# 3) ê²€ì¦ì „ëµ (TimeSeriesSplit) & ëª¨ë¸ í•™ìŠµ (C,D)
# =========================
def smape(y_true, y_pred):
    # SMAPE: 2*|y - yhat| / (|y|+|yhat|), 0/0 ë°©ì§€
    denom = (np.abs(y_true) + np.abs(y_pred)).clip(1e-8, None)
    return np.mean(2.0 * np.abs(y_true - y_pred) / denom)

def rmse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))

# ì •ë ¬ í›„ ì¸ë±ìŠ¤ ê¸°ë°˜ ì‹œê³„ì—´ ë¶„í• 
train = train.sort_values(["date","product_name"]).reset_index(drop=True)

tscv = TimeSeriesSplit(n_splits=N_SPLITS)  # ë‹¨ìˆœ ì‹œê°„ìˆœ ë¶„í• (ìƒ˜í”Œ ìˆœì„œê°€ ì‹œê°„ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•¨)

oof_pred = np.zeros(len(train))
test_pred_seeds = np.zeros((len(test_feat), len(SEEDS)))
feature_importances = []

for s_idx, seed in enumerate(SEEDS):
    print(f"\n===== SEED {seed} =====")
    fold_preds = np.zeros(len(train))
    test_pred = np.zeros(len(test_feat))

    for fold, (trn_idx, val_idx) in enumerate(tscv.split(train)):
        trn = train.iloc[trn_idx]
        val = train.iloc[val_idx]

        X_trn = trn[num_cols]
        y_trn = trn[TARGET_COL].values
        X_val = val[num_cols]
        y_val = val[TARGET_COL].values

        model = LGBMRegressor(
            objective="rmse",
            n_estimators=5000,
            learning_rate=0.03,
            num_leaves=63,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.3,
            random_state=seed,
            n_jobs=-1,
            verbose=-1,
        )

        model.fit(
            X_trn, y_trn,
            eval_set=[(X_val, y_val)],
            callbacks=[
                early_stopping(stopping_rounds=200, verbose=False),
                log_evaluation(200)
            ]
        )

        val_pred = model.predict(X_val)
        fold_preds[val_idx] = val_pred
        test_pred += model.predict(test_feat[num_cols]) / N_SPLITS

        # í”¼ì²˜ ì¤‘ìš”ë„ ìˆ˜ì§‘
        feature_importances.append(
            pd.DataFrame({"feature": num_cols, "importance": model.feature_importances_, "seed": seed, "fold": fold})
        )

        print(f"[FOLD {fold}] RMSE={rmse(y_val, val_pred):.4f} | SMAPE={smape(y_val, val_pred):.4f}")

    # seedë³„ OOF/TEST ì¶•ì 
    oof_pred += fold_preds / len(SEEDS)
    test_pred_seeds[:, s_idx] = test_pred

    print(f"[SEED {seed}] OOF RMSE={rmse(train[TARGET_COL], fold_preds):.4f} | SMAPE={smape(train[TARGET_COL], fold_preds):.4f}")

# ì „ì²´ OOF
print("\n===== OVERALL (OOF across seeds) =====")
print(f"OOF RMSE={rmse(train[TARGET_COL], oof_pred):.4f} | SMAPE={smape(train[TARGET_COL], oof_pred):.4f}")

# =========================
# 4) ì˜ˆì¸¡/ì œì¶œ ìƒì„± (E)
# =========================
# TEST ì˜ˆì¸¡: seed í‰ê· 
test_pred = test_pred_seeds.mean(axis=1)
test_out = test_feat[KEY_COLS].copy()
test_out["prediction"] = test_pred

# (ì„ íƒ) product_infoë¡œ id ë§¤í•‘ í•„ìš” ì‹œ
# product_info ì˜ˆì‹œì— product_id, product_name ë“±ì´ ìˆìœ¼ë©´ ì¡°ì¸í•´ id ìƒì„± ê°€ëŠ¥
if os.path.exists(PRODUCT_INFO_PATH):
    info = pd.read_csv(PRODUCT_INFO_PATH)
    # ê°€ì •: info ì•ˆì— product_nameì´ ìˆê³ , sample_submissionì˜ idê°€ (product_id + date)ì—ì„œ íŒŒìƒëœë‹¤ë©´ í•˜ë‹¨ ë¡œì§ ì¡°ì •
    # test_out = test_out.merge(info[["product_name","product_id"]], on="product_name", how="left")

# sample_submissionê³¼ ë§¤í•‘
if os.path.exists(SAMPLE_SUB_PATH):
    sub = pd.read_csv(SAMPLE_SUB_PATH)
    # ëŒ€íšŒë³„ í¬ë§·ì´ ë‹¤ë¥´ë¯€ë¡œ, ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µí•©ë‹ˆë‹¤.
    # ê°€ì • 1) sample_submissionì— product_name / date ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°:
    candidates = [c.lower() for c in sub.columns]
    has_pn = "product_name" in sub.columns
    has_date = "date" in sub.columns

    if has_pn and has_date:
        sub["date"] = pd.to_datetime(sub["date"]).dt.to_period("M").dt.to_timestamp()
        sub = sub.merge(test_out, on=["product_name","date"], how="left")
        # ëŒ€íšŒ íƒ€ê¹ƒ ì»¬ëŸ¼ëª…ì— ë§ì¶° rename
        if "target" in sub.columns:
            sub["target"] = sub["prediction"]
        elif "quantity" in sub.columns:
            sub["quantity"] = sub["prediction"]
        else:
            sub["prediction"] = sub["prediction"]
    else:
        # ê°€ì • 2) idë§Œ ìˆê³ , ë³„ë„ í‚¤ê°€ ì—†ë‹¤ë©´ ì„ì‹œë¡œ product_name/date ìˆœ ì •ë ¬ í›„ ëŒ€ì… (ê¶Œì¥X)
        print("[WARN] sample_submissionì— í‚¤ ì»¬ëŸ¼(product_name/date)ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¨ìˆœ ëŒ€ì… ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sub = sub.copy()
        sub["prediction"] = 0.0
        # í•„ìš”ì‹œ ì—¬ê¸°ì„œ ë³„ë„ ë§¤í•‘ ë¡œì§ êµ¬í˜„
    sub_path = os.path.join(SAVE_DIR, "submission.csv")
    sub.to_csv(sub_path, index=False, encoding="utf-8-sig")
    print("âœ… ì œì¶œ íŒŒì¼ ì €ì¥:", sub_path)

# ë³´ì¡° ì•„ì›ƒí’‹ë“¤ ì €ì¥
oof_df = train[KEY_COLS + [TARGET_COL]].copy()
oof_df["oof_pred"] = oof_pred
oof_path = os.path.join(SAVE_DIR, "oof_predictions.csv")
oof_df.to_csv(oof_path, index=False, encoding="utf-8-sig")
print("âœ… OOF ì €ì¥:", oof_path)

# í”¼ì²˜ ì¤‘ìš”ë„
if len(feature_importances):
    fi = pd.concat(feature_importances, ignore_index=True)
    fi_agg = (fi.groupby("feature")["importance"].mean().sort_values(ascending=False).reset_index())
    fi_path = os.path.join(SAVE_DIR, "feature_importance.csv")
    fi.to_csv(fi_path, index=False, encoding="utf-8-sig")
    fi_agg_path = os.path.join(SAVE_DIR, "feature_importance_agg.csv")
    fi_agg.to_csv(fi_agg_path, index=False, encoding="utf-8-sig")
    print("âœ… ì¤‘ìš”ë„ ì €ì¥:", fi_path, ",", fi_agg_path)

print("ğŸ‰ Done.")
