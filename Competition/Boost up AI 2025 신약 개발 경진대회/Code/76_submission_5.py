import os
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.nn import Linear, ModuleList, ReLU, Dropout, BatchNorm1d # BatchNorm1d ì¶”ê°€
from torch_geometric.nn import GATv2Conv, global_mean_pool, GraphNorm, JumpingKnowledge
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
from rdkit import Chem
from rdkit.Chem import Descriptors, MACCSkeys
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel
import optuna
import joblib
import warnings

# RDKit Deprecation ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
warnings.filterwarnings("ignore", category=DeprecationWarning, module='rdkit.Chem.MACCSkeys')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
os.makedirs("./_save", exist_ok=True)

# 1. ğŸ“¦ ë°ì´í„° ë¡œë“œ
train = pd.read_csv('./_data/dacon/new/train.csv')
test = pd.read_csv('./_data/dacon/new/test.csv')
submission = pd.read_csv('./_data/dacon/new/sample_submission.csv')

# 2. ğŸ§ª RDKit Descriptors + MACCS Keys ê³„ì‚° í•¨ìˆ˜
def calc_rdkit_maccs(smiles_list):
    rdkit_desc_list = []
    maccs_keys_list = []
    desc_names = [d[0] for d in Descriptors._descList]
    for smi in smiles_list:
        mol = Chem.MolFromSmiles(smi)
        if mol:
            rdkit_desc = [desc(mol) for _, desc in Descriptors._descList]
            maccs = list(MACCSkeys.GenMACCSKeys(mol))
        else:
            rdkit_desc = [0] * len(desc_names)
            maccs = [0] * 167
        rdkit_desc_list.append(rdkit_desc)
        maccs_keys_list.append(maccs)
    rdkit_arr = np.array(rdkit_desc_list)
    maccs_arr = np.array(maccs_keys_list)
    return rdkit_arr, maccs_arr

# 3. ğŸ§¬ GNN ì…ë ¥ ë³€í™˜
def mol_to_graph_data_obj(smiles, global_feat):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    atom_feats = []
    for atom in mol.GetAtoms():
        atom_feats.append([
            atom.GetAtomicNum(),
            int(atom.GetIsAromatic()),
            atom.GetDegree(),
            atom.GetFormalCharge(),
            atom.GetTotalNumHs()
        ])
    edge_index = []
    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()
        edge_index += [[i, j], [j, i]]
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    x = torch.tensor(atom_feats, dtype=torch.float)
    data = Data(x=x, edge_index=edge_index)
    data.global_feat = torch.tensor(global_feat, dtype=torch.float)
    return data

# 4. ğŸ“Š í‰ê°€ í•¨ìˆ˜
def compute_metrics(y_true, y_pred):
    # NaNì´ í¬í•¨ëœ ì˜ˆì¸¡ê°’ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    y_pred_filtered = np.array(y_pred)
    y_true_filtered = np.array(y_true)
    
    # NaNì´ ì•„ë‹Œ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    not_nan_indices = ~np.isnan(y_pred_filtered)
    
    if not np.any(not_nan_indices): # ëª¨ë“  ì˜ˆì¸¡ê°’ì´ NaNì¸ ê²½ìš°
        return 1.0, 0.0, 0.0 # RMSEë¥¼ ìµœëŒ€ê°’ìœ¼ë¡œ, ìƒê´€ê³„ìˆ˜ì™€ ìŠ¤ì½”ì–´ë¥¼ 0ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ Optunaê°€ ì´ Trialì„ í”¼í•˜ê²Œ í•¨
    
    y_true_clean = y_true_filtered[not_nan_indices]
    y_pred_clean = y_pred_filtered[not_nan_indices]
    
    if len(y_true_clean) == 0: # ìœ íš¨í•œ ê°’ì´ ì—†ëŠ” ê²½ìš°
        return 1.0, 0.0, 0.0

    rmse = mean_squared_error(y_true_clean, y_pred_clean, squared=False)
    # y_true_cleanì— ë‹¨ì¼ ê°’ë§Œ ìˆëŠ” ê²½ìš° pearsonr ì˜¤ë¥˜ ë°©ì§€
    if len(np.unique(y_true_clean)) == 1 or len(np.unique(y_pred_clean)) == 1:
        corr = 0.0 # ìƒê´€ê³„ìˆ˜ ê³„ì‚° ë¶ˆê°€
    else:
        corr = np.corrcoef(y_true_clean, y_pred_clean)[0, 1]
        if np.isnan(corr): # ìƒê´€ê³„ìˆ˜ê°€ NaNì´ ë˜ëŠ” ê²½ìš° (ì˜ˆ: ëª¨ë“  ì˜ˆì¸¡ê°’ì´ ê°™ì„ ë•Œ)
            corr = 0.0

    # ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë¡œ RMSEë¥¼ ì •ê·œí™”í•˜ì—¬ score ê³„ì‚° ì‹œ ë” í•©ë¦¬ì ì¸ ê°’ì„ ì–»ë„ë¡ í•©ë‹ˆë‹¤.
    # train['Inhibition']ì˜ ì „ì²´ ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”
    data_range = train['Inhibition'].max() - train['Inhibition'].min()
    if data_range == 0: # ë°ì´í„° ë²”ìœ„ê°€ 0ì¸ ê²½ìš° (ëª¨ë“  ê°’ì´ ë™ì¼)
        normalized_rmse = 0.0
    else:
        normalized_rmse = rmse / data_range
        
    score = 0.5 * (1 - min(normalized_rmse, 1)) + 0.5 * max(corr, 0)
    return rmse, corr, score

# 5. ğŸ§  ëª¨ë¸ ì •ì˜
class GNNModel(torch.nn.Module):
    def __init__(self, in_dim, global_dim, hidden, layers, dropout):
        super().__init__()
        self.convs = ModuleList()
        self.norms = ModuleList()
        # ì…ë ¥ ë ˆì´ì–´
        self.convs.append(GATv2Conv(in_dim, hidden))
        self.norms.append(GraphNorm(hidden)) # GraphNormì€ Node ë ˆë²¨ì—ì„œ ì‘ë™í•˜ë¯€ë¡œ hidden ì°¨ì›
        
        # ì¤‘ê°„ ë ˆì´ì–´
        for _ in range(layers - 1):
            self.convs.append(GATv2Conv(hidden, hidden))
            self.norms.append(GraphNorm(hidden))

        self.jump = JumpingKnowledge("cat") # ê° ë ˆì´ì–´ì˜ í’€ë§ëœ ì¶œë ¥ì„ concat
        
        # ë§ˆì§€ë§‰ Linear ë ˆì´ì–´
        self.fc1 = Linear(hidden * layers + global_dim, hidden)
        self.fc2 = Linear(hidden, 1)
        
        self.dropout = Dropout(dropout)
        self.relu = ReLU()

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        
        outs = []
        for conv, norm in zip(self.convs, self.norms):
            x = conv(x, edge_index)
            # GATv2Conv í›„ì— ë°°ì¹˜ ì •ê·œí™” (GraphNorm) ë° ReLU ì ìš©
            x = self.relu(norm(x)) 
            
            # ê° conv ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í’€ë§í•˜ì—¬ ì €ì¥
            outs.append(global_mean_pool(x, batch))
        
        # ëª¨ë“  ë ˆì´ì–´ì˜ í’€ë§ëœ ì¶œë ¥ì„ ê²°í•©
        h = torch.cat(outs, dim=1)
        
        # Global Feature ê°€ì ¸ì˜¤ê¸°
        g = data.global_feat.to(h.device)

        # gì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ hì™€ ì¼ì¹˜ì‹œí‚¤ê¸° (gê°€ ìŠ¤ì¹¼ë¼ë¡œ ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„)
        if g.dim() == 1 or g.size(0) != h.size(0):
            g = g.view(h.size(0), -1)
        
        # GNN ì¶œë ¥ê³¼ Global Featureë¥¼ ê²°í•©
        out = torch.cat([h, g], dim=1)
        
        # ìµœì¢… FC ë ˆì´ì–´
        out = self.dropout(self.relu(self.fc1(out)))
        return self.fc2(out)

# 6. âœ¨ SHAP ê¸°ë°˜ MACCS ì„ íƒ
rdkit_train, maccs_train = calc_rdkit_maccs(train['Canonical_Smiles'])
scaler = StandardScaler()
rdkit_scaled = scaler.fit_transform(rdkit_train)
selector = SelectFromModel(GradientBoostingRegressor(), max_features=30)
selector.fit(maccs_train, train['Inhibition'])
maccs_top = selector.transform(maccs_train)
X_global = np.concatenate([rdkit_scaled, maccs_top], axis=1)
y = train['Inhibition'].values

# 7. ğŸ” Graph ë³€í™˜
graph_list = []
for smi, g_feat in zip(train['Canonical_Smiles'], X_global):
    graph = mol_to_graph_data_obj(smi, g_feat)
    if graph:
        graph.y = torch.tensor([y[len(graph_list)]], dtype=torch.float)
        graph_list.append(graph)

# 8. ğŸ¯ Optuna ìµœì í™”
def objective(trial):
    hidden = trial.suggest_categorical("hidden", [128, 256])
    layers = trial.suggest_int("layers", 2, 4)
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    # í•™ìŠµë¥  ë²”ìœ„ë¥¼ ë” ë‚®ê²Œ ì¡°ì •
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True) 
    batch_size = trial.suggest_categorical("batch", [32, 64])
    patience = trial.suggest_int("patience", 10, 30)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = []
    
    for train_idx, val_idx in kf.split(graph_list):
        train_dataset = [graph_list[i] for i in train_idx]
        val_dataset = [graph_list[i] for i in val_idx]
        
        model = GNNModel(5, X_global.shape[1], hidden, layers, dropout).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        best_score, best_epoch = 0, 0
        
        for epoch in range(1000): # ì—í­ ìˆ˜ ê³ ì •
            model.train()
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                pred = model(batch)
                loss = F.mse_loss(pred.squeeze(), batch.y)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì¶”ê°€ (ì˜µì…˜, í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) 
                
                loss.backward()
                optimizer.step()
            
            model.eval()
            val_loader = DataLoader(val_dataset, batch_size=128)
            preds, trues = [], []
            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(device)
                    pred_batch = model(batch)
                    preds += pred_batch.squeeze().cpu().numpy().tolist()
                    trues += batch.y.cpu().numpy().tolist()
            
            # compute_metrics í•¨ìˆ˜ì—ì„œ NaN ì²˜ë¦¬ ë¡œì§ì´ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œ Noneì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            rmse, corr, score = compute_metrics(trues, preds)
            
            # scoreê°€ NaNì´ë©´ Optunaì— ì „ë‹¬ë˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
            if np.isnan(score):
                return None # Optunaê°€ ì´ ì‹œë„ë¥¼ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ë„ë¡ í•¨
            
            if score > best_score:
                best_score, best_epoch = score, epoch
            elif epoch - best_epoch > patience:
                break
        scores.append(best_score)
    
    # K-fold êµì°¨ ê²€ì¦ ì¤‘ í•˜ë‚˜ë¼ë„ NaNì´ ë°œìƒí•˜ì—¬ Noneì´ ë°˜í™˜ë˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„
    if any(np.isnan(s) for s in scores):
        return None

    return np.mean(scores)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30) # n_trialsë¥¼ 30ìœ¼ë¡œ ì„¤ì •
joblib.dump(study, "./_save/optuna_study_gnn.pkl")

# 9. ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
best_params = study.best_params # Optuna ìŠ¤í„°ë””ì—ì„œ best_trialì´ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ ì£¼ì˜
model = GNNModel(5, X_global.shape[1], best_params['hidden'], best_params['layers'], best_params['dropout']).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])
train_loader = DataLoader(graph_list, batch_size=best_params['batch'], shuffle=True)

# ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì—í­ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜, ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤)
for epoch in range(300): # ì˜ˆì‹œ ì—í­ ìˆ˜
    model.train()
    for batch in train_loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        pred = model(batch)
        loss = F.mse_loss(pred.squeeze(), batch.y)
        loss.backward()
        optimizer.step()

torch.save(model.state_dict(), "./_save/best_gnn_model.pt")

# 10. ğŸ“¤ ì œì¶œ íŒŒì¼ ìƒì„±
rdkit_test, maccs_test = calc_rdkit_maccs(test['Canonical_Smiles'])
scaler_test = StandardScaler() # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ scalerë¡œ transformë§Œ í•´ì•¼ í•©ë‹ˆë‹¤.
rdkit_test_scaled = scaler.fit_transform(rdkit_test) # ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤. í›ˆë ¨ ë°ì´í„°ë¡œ fitëœ scaler ì‚¬ìš©
# ì •í™•í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©: rdkit_scaled = scaler.fit_transform(rdkit_train) ì—ì„œ fitëœ scalerë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
# ì´ë¯¸ ìœ„ì— scaler.fit_transform(rdkit_train)ì´ ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” transformë§Œ í•©ë‹ˆë‹¤.
rdkit_test_scaled = scaler.transform(rdkit_test) 


maccs_test_top = selector.transform(maccs_test)
X_test_global = np.concatenate([rdkit_test_scaled, maccs_test_top], axis=1)

# ìœ íš¨í•œ ê·¸ë˜í”„ ê°ì²´ë§Œ í•„í„°ë§
test_graphs = [mol_to_graph_data_obj(smi, feat) for smi, feat in zip(test['Canonical_Smiles'], X_test_global)]
test_graphs = [g for g in test_graphs if g is not None]

test_loader = DataLoader(test_graphs, batch_size=128)

model.eval()
preds = []
with torch.no_grad():
    for batch in test_loader:
        batch = batch.to(device)
        preds += model(batch).squeeze().cpu().numpy().tolist()

submission['Predicted'] = preds
submission.to_csv('./_save/gnn_submission.csv', index=False)
print(f"Submission saved to ./_save/gnn_submission.csv")