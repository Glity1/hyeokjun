import numpy as np
import pandas as pd
import os, platform
from tqdm import tqdm
from rdkit import Chem
from rdkit.Chem import Descriptors, MACCSkeys, AllChem
from rdkit import DataStructs
import torch
from torch.nn import Linear, Module, ReLU, Dropout, BatchNorm1d
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GATv2Conv, global_mean_pool
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
from torch.optim.lr_scheduler import ReduceLROnPlateau 

# Optuna ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import optuna.visualization
import plotly.io as pio # ì´ë¯¸ì§€ ì €ì¥ì„ ìœ„í•´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ê´€ë¦¬: Config í´ë˜ìŠ¤ (ì›ë˜ ìœ„ì¹˜) ---
class Config:
    seed = 42
    use_topN = 30 # Morgan TopN í”¼ì²˜ ê°œìˆ˜ (ë§Œì•½ íŒŒì¼ ì—†ìœ¼ë©´ ì „ì²´ Morgan FP ì‚¬ìš©)
    batch_size = 64
    epochs = 100 # ê° Optuna trial ë‚´ K-Fold í•™ìŠµì˜ ìµœëŒ€ ì—í¬í¬ ìˆ˜ ë° ìµœì¢… ëª¨ë¸ í•™ìŠµì˜ ìµœëŒ€ ì—í¬í¬ ìˆ˜
    patience = 7 # Early Stopping patience

    # GNN ëª¨ë¸ ê´€ë ¨ íŒŒë¼ë¯¸í„°
    node_feat_dim = 14 # 14ê°œ í”¼ì²˜ ìœ ì§€
    edge_feat_dim = 9
    
    # Global feature ê´€ë ¨
    # ì´ˆê¸°ì—ëŠ” ì„ì‹œ ê°’ì„ ì„¤ì • (ë‚˜ì¤‘ì— ì‹¤ì œ rdkit_desc_list ê¸¸ì´ë¡œ ì—…ë°ì´íŠ¸ë  ê²ƒì„)
    rdkit_desc_dim = 20 # ì´ ê°’ì€ ì•„ë˜ì—ì„œ ì‹¤ì œ ê¸¸ì´ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
    maccs_keys_dim = 167
    
    # K-Fold ì„¤ì •
    n_splits = 3 # K-Fold ë¶„í•  ê°œìˆ˜ë¥¼ 3ìœ¼ë¡œ ì¡°ì • (íŠœë‹ ì‹œê°„ì„ ê³ ë ¤í•˜ì—¬ ìœ ì§€)

# ì„¤ì •
np.random.seed(Config.seed)
torch.manual_seed(Config.seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
os.makedirs('./_save', exist_ok=True)

# í•œê¸€ í°íŠ¸ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ìˆ˜ì •)
plt.rcParams['font.family'] = 'Malgun Gothic' if platform.system() == 'Windows' else 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ë¡œë“œ (ì´ì „ê³¼ ë™ì¼)
try:
    # ë°ì´í„° ê²½ë¡œë¥¼ ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”.
    train_df = pd.read_csv('./_data/dacon/new/train.csv').drop_duplicates('Canonical_Smiles').reset_index(drop=True)
    test_df = pd.read_csv('./_data/dacon/new/test.csv')
    submit_df = pd.read_csv('./_data/dacon/new/sample_submission.csv')
except FileNotFoundError as e:
    print(f"ì˜¤ë¥˜: í•„ìš”í•œ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {e}")
    exit()

smiles_train_raw_initial = train_df['Canonical_Smiles'].tolist()
y_original_initial = train_df['Inhibition'].values # ì›ë³¸ ìŠ¤ì¼€ì¼ì˜ Inhibition ê°’
y_transformed_initial = y_original_initial # log1p ë³€í™˜ ì œê±° ìœ ì§€

smiles_test_raw_initial = test_df['Canonical_Smiles'].tolist()

# --- Morgan FP --- (ì´ì „ê³¼ ë™ì¼)
def get_morgan_fp(smiles, radius=2, n_bits=2048):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    try:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
        arr = np.zeros((n_bits,), dtype=np.float32)
        DataStructs.ConvertToNumpyArray(fp, arr)
        return arr
    except Exception as e:
        return None

print("Morgan Fingerprint ìƒì„± ì¤‘ (í›ˆë ¨ ë°ì´í„°)...")
morgan_train_raw = [get_morgan_fp(s) for s in tqdm(smiles_train_raw_initial, desc="Train Morgan FP")]

# ìœ íš¨í•œ Morgan FPë¥¼ ê°€ì§„ í›ˆë ¨ ë°ì´í„°ë§Œ í•„í„°ë§
valid_train_indices_morgan = [i for i, fp in enumerate(morgan_train_raw) if fp is not None]
smiles_train_filtered_by_morgan = [smiles_train_raw_initial[i] for i in valid_train_indices_morgan]
morgan_train = np.array([morgan_train_raw[i] for i in valid_train_indices_morgan])
y_transformed_filtered_for_graph = y_transformed_initial[valid_train_indices_morgan]


print("Morgan Fingerprint ìƒì„± ì¤‘ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)...")
morgan_test_raw = [get_morgan_fp(s) for s in tqdm(smiles_test_raw_initial, desc="Test Morgan FP")]

valid_test_indices_morgan = [i for i, fp in enumerate(morgan_test_raw) if fp is not None]
smiles_test_filtered_by_morgan = [smiles_test_raw_initial[i] for i in valid_test_indices_morgan]
morgan_test = np.array([morgan_test_raw[i] for i in valid_test_indices_morgan])

# ì›ë³¸ test_df ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ í•„í„°ë§ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥
test_original_indices_filtered = [test_df.index[i] for i in valid_test_indices_morgan]

print(f"ìœ íš¨í•œ Morgan Train ë°ì´í„°ì…‹ í¬ê¸°: {len(morgan_train)}")
print(f"ìœ íš¨í•œ Morgan Test ë°ì´í„°ì…‹ í¬ê¸°: {len(morgan_test)}")

# Morgan TopN í”¼ì²˜ ë¡œë“œ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ì „ì²´ Morgan FP ì‚¬ìš©)
top_idx_path = f'./_save/morgan_top{Config.use_topN}_from_combined.npy'
if not os.path.exists(top_idx_path):
    print(f"ê²½ê³ : {top_idx_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Morgan TopN í”¼ì²˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("Morgan TopN ëŒ€ì‹  ì „ì²´ Morgan FPë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê²½ìš° Config.use_topNì´ ë³€ê²½ë©ë‹ˆë‹¤.")
    if morgan_train.shape[0] > 0:
        morgan_train_top = morgan_train
        morgan_test_top = morgan_test
        Config.use_topN = morgan_train.shape[1] # ì‹¤ì œ ì‚¬ìš©ë  Morgan FP ì°¨ì›
    else: # morgan_train ìì²´ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
        morgan_train_top = np.array([])
        morgan_test_top = np.array([])
        Config.use_topN = 0
else:
    top_idx = np.load(top_idx_path)
    if len(top_idx) == 0: # top_idxê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° (ì˜ˆ: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì‹œ ëª¨ë“  í”¼ì²˜ê°€ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨)
        print(f"ê²½ê³ : {top_idx_path} íŒŒì¼ì— ìœ íš¨í•œ TopN ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. Morgan FPë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        morgan_train_top = np.array([])
        morgan_test_top = np.array([])
        Config.use_topN = 0
    else:
        Config.use_topN = len(top_idx) 
        morgan_train_top = morgan_train[:, top_idx]
        morgan_test_top = morgan_test[:, top_idx]
print(f"Morgan í”¼ì²˜ ì°¨ì›: {Config.use_topN}")


# --- RDKit Descriptors ëª©ë¡ ì •ì˜ ---
# Descriptors.NumRings ì œê±° ë° Config.rdkit_desc_dim ì—…ë°ì´íŠ¸ ë¡œì§ ë°˜ì˜
rdkit_desc_list = [
    Descriptors.MolWt, Descriptors.MolLogP, Descriptors.TPSA,
    Descriptors.NumRotatableBonds, Descriptors.NumHAcceptors, Descriptors.NumHDonors,
    Descriptors.HeavyAtomCount, Descriptors.FractionCSP3,
    Descriptors.NHOHCount, Descriptors.NOCount, Descriptors.NumAliphaticRings,
    Descriptors.NumAromaticRings, Descriptors.NumSaturatedRings, Descriptors.ExactMolWt,
    Descriptors.MolMR, Descriptors.LabuteASA, Descriptors.BalabanJ,
    Descriptors.MaxPartialCharge, Descriptors.MinPartialCharge
]

# rdkit_desc_listê°€ ì •ì˜ëœ í›„, Config í´ë˜ìŠ¤ì˜ rdkit_desc_dimì„ ì—…ë°ì´íŠ¸
Config.rdkit_desc_dim = len(rdkit_desc_list)
print(f"RDKit Descriptors ì°¨ì› (ì—…ë°ì´íŠ¸): {Config.rdkit_desc_dim}")


def get_rdkit_and_maccs(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None, None
    try:
        desc = [f(mol) for f in rdkit_desc_list]
        desc = np.nan_to_num(desc, nan=0.0).astype(np.float32)
        maccs = MACCSkeys.GenMACCSKeys(mol)
        maccs_arr = np.zeros((Config.maccs_keys_dim,), dtype=np.int8)
        DataStructs.ConvertToNumpyArray(maccs, maccs_arr)
        return desc, maccs_arr.astype(np.float32)
    except Exception as e:
        return None, None

# --- Node Feature (14ê°œ í”¼ì²˜) ---
def atom_features(atom):
    return [
        atom.GetAtomicNum(),  # ì›ì ë²ˆí˜¸
        atom.GetTotalNumHs(),  # ìˆ˜ì†Œ ì›ì ì´ ê°œìˆ˜
        int(atom.GetHybridization()),  # í˜¼ì„± ê¶¤ë„ (sp, sp2, sp3 ë“±)
        atom.GetFormalCharge(),  # í˜•ì‹ ì „í•˜
        int(atom.GetIsAromatic()),  # ë°©í–¥ì¡± ì—¬ë¶€
        int(atom.IsInRing()),  # ê³ ë¦¬ ë‚´ í¬í•¨ ì—¬ë¶€
        atom.GetDegree(),  # ì›ìì˜ ì°¨ìˆ˜ (ì—°ê²°ëœ ë¹„ìˆ˜ì†Œ ì›ì ìˆ˜)
        atom.GetExplicitValence(),  # ëª…ì‹œì  ì›ìê°€
        atom.GetImplicitValence(),  # ë¬µì‹œì  ì›ìê°€
        atom.GetMass(), # ì›ì ì§ˆëŸ‰
        atom.GetNumRadicalElectrons(), # ë¼ë””ì¹¼ ì „ì ìˆ˜
        atom.GetTotalValence(), # ì´ ì›ìê°€
        atom.GetChiralTag(), # ì¹´ì´ë„ íƒœê·¸ (ì •ìˆ˜ ê°’)
        atom.GetIsotope(), # ë™ìœ„ì›ì†Œ (ì •ìˆ˜ ê°’)
    ]

# --- Stereo One-hot ---
def stereo_onehot(bond):
    onehot = [0] * 5
    stereo = int(bond.GetStereo())
    if 0 <= stereo < 5:
        onehot[stereo] = 1
    return onehot

# --- Edge Feature ---
def get_bond_features(bond):
    return [
        bond.GetBondTypeAsDouble(),
        bond.GetIsConjugated(),
        bond.GetIsAromatic(),
        *stereo_onehot(bond),
        bond.IsInRing(),
    ]

# --- Graph ë³€í™˜ í•¨ìˆ˜ (original_y_idx ì¶”ê°€) ---
def smiles_to_graph_with_global(smiles, global_feat, label=None, original_y_idx=None):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None

    try:
        x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)

        edge_indices, edge_attrs = [], []
        for bond in mol.GetBonds():
            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
            feat = get_bond_features(bond)
            edge_indices += [[i, j], [j, i]]
            edge_attrs += [feat, feat]

        if not edge_indices:
            edge_index = torch.empty((2, 0), dtype=torch.long)
            edge_attr = torch.empty((0, Config.edge_feat_dim), dtype=torch.float)
        else:
            edge_index = torch.tensor(edge_indices).t().contiguous()
            edge_attr = torch.tensor(edge_attrs, dtype=torch.float)

        global_feat_tensor = torch.tensor(global_feat, dtype=torch.float).unsqueeze(0)
        y_tensor = torch.tensor([label], dtype=torch.float) if label is not None else None

        data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, global_feat=global_feat_tensor, y=y_tensor)
        data_obj.idx_in_original_y = original_y_idx # ì›ë³¸ y ë°°ì—´ì—ì„œì˜ ì¸ë±ìŠ¤ ì €ì¥
        return data_obj
    except Exception as e:
        return None

# --- í†µí•© í”¼ì²˜ êµ¬ì„± ë° Graph ë°ì´í„° ìƒì„± ---
print("\nê·¸ë˜í”„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")

# Train ë°ì´í„°ì˜ RDKit Descriptorsì™€ MACCS Keys ì¶”ì¶œ
train_rdkit_descs_raw, train_maccs_keys_raw = [], []
graph_creation_valid_indices_in_filtered_smiles = [] 

for i, s in enumerate(tqdm(smiles_train_filtered_by_morgan, desc="Extracting Train RDKit/MACCS & Graph")):
    rdkit_desc, maccs_keys = get_rdkit_and_maccs(s)
    
    if rdkit_desc is None or len(rdkit_desc) != Config.rdkit_desc_dim or maccs_keys is None or len(maccs_keys) != Config.maccs_keys_dim:
        continue

    if i >= len(morgan_train_top) or morgan_train_top[i] is None or (Config.use_topN > 0 and morgan_train_top[i].size == 0):
        continue

    train_rdkit_descs_raw.append(rdkit_desc)
    train_maccs_keys_raw.append(maccs_keys)
    graph_creation_valid_indices_in_filtered_smiles.append(i)

morgan_train_top_final = np.array([morgan_train_top[idx] for idx in graph_creation_valid_indices_in_filtered_smiles])
smiles_train_final = [smiles_train_filtered_by_morgan[idx] for idx in graph_creation_valid_indices_in_filtered_smiles]
y_transformed_final = np.array([y_transformed_filtered_for_graph[idx] for idx in graph_creation_valid_indices_in_filtered_smiles])
y_original_indices_for_graph_final = [valid_train_indices_morgan[idx] for idx in graph_creation_valid_indices_in_filtered_smiles] 


# Test ë°ì´í„°ì˜ RDKit Descriptorsì™€ MACCS Keys ì¶”ì¶œ
test_rdkit_descs_raw, test_maccs_keys_raw = [], []
graph_creation_valid_indices_in_test_filtered_smiles = [] 

for i, s in enumerate(tqdm(smiles_test_filtered_by_morgan, desc="Extracting Test RDKit/MACCS & Graph")):
    rdkit_desc, maccs_keys = get_rdkit_and_maccs(s)
    
    if rdkit_desc is None or len(rdkit_desc) != Config.rdkit_desc_dim or maccs_keys is None or len(maccs_keys) != Config.maccs_keys_dim:
        continue

    if i >= len(morgan_test_top) or morgan_test_top[i] is None or (Config.use_topN > 0 and morgan_test_top[i].size == 0):
        continue

    test_rdkit_descs_raw.append(rdkit_desc)
    test_maccs_keys_raw.append(maccs_keys)
    graph_creation_valid_indices_in_test_filtered_smiles.append(i)

morgan_test_top_final = np.array([morgan_test_top[idx] for idx in graph_creation_valid_indices_in_test_filtered_smiles])
smiles_test_final = [smiles_test_filtered_by_morgan[idx] for idx in graph_creation_valid_indices_in_test_filtered_smiles]
test_original_indices_final = [test_original_indices_filtered[idx] for idx in graph_creation_valid_indices_in_test_filtered_smiles]


# RDKit Descriptorsë§Œ StandardScaler ì ìš© (ë°ì´í„°ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
train_rdkit_descs_np = np.array(train_rdkit_descs_raw)
test_rdkit_descs_np = np.array(test_rdkit_descs_raw)

if train_rdkit_descs_np.shape[0] > 0:
    scaler = StandardScaler()
    train_rdkit_descs_scaled = scaler.fit_transform(train_rdkit_descs_np)
    test_rdkit_descs_scaled = scaler.transform(test_rdkit_descs_np)
else:
    train_rdkit_descs_scaled = np.array([]) 
    test_rdkit_descs_scaled = np.array([])


# ìµœì¢… Graph Data ê°ì²´ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
train_data = []
for i in tqdm(range(len(smiles_train_final)), desc="Creating Train Graphs"):
    components = []
    if train_rdkit_descs_scaled.shape[0] > 0 and i < train_rdkit_descs_scaled.shape[0]:
        components.append(train_rdkit_descs_scaled[i])
    if train_maccs_keys_raw and i < len(train_maccs_keys_raw):
        components.append(train_maccs_keys_raw[i])
    if morgan_train_top_final.shape[0] > 0 and i < morgan_train_top_final.shape[0]:
        components.append(morgan_train_top_final[i])
    
    if not components:
        continue

    global_feat_combined = np.concatenate(components)

    data_obj = smiles_to_graph_with_global(
        smiles_train_final[i], 
        global_feat_combined, 
        y_transformed_final[i], 
        original_y_idx=y_original_indices_for_graph_final[i]
    )
    
    if data_obj is not None:
        train_data.append(data_obj)

test_data_with_indices = []
for i in tqdm(range(len(smiles_test_final)), desc="Creating Test Graphs"):
    components = []
    if test_rdkit_descs_scaled.shape[0] > 0 and i < test_rdkit_descs_scaled.shape[0]:
        components.append(test_rdkit_descs_scaled[i])
    if test_maccs_keys_raw and i < len(test_maccs_keys_raw):
        components.append(test_maccs_keys_raw[i])
    if morgan_test_top_final.shape[0] > 0 and i < morgan_test_top_final.shape[0]:
        components.append(morgan_test_top_final[i])

    if not components:
        continue

    global_feat_combined = np.concatenate(components)

    graph_data = smiles_to_graph_with_global(smiles_test_final[i], global_feat_combined)
    if graph_data is not None:
        test_data_with_indices.append((graph_data, test_original_indices_final[i]))

test_data = [d[0] for d in test_data_with_indices]
test_submission_original_indices = [d[1] for d in test_data_with_indices]

print(f"ìµœì¢… Train Graph ë°ì´í„° ê°œìˆ˜: {len(train_data)}")
print(f"ìµœì¢… Test Graph ë°ì´í„° ê°œìˆ˜: {len(test_data)}")

# --- GNN ëª¨ë¸ ì •ì˜ (Optunaë¡œë¶€í„° íŒŒë¼ë¯¸í„° ë°›ë„ë¡ ìˆ˜ì •) ---
class GATv2WithGlobal(Module):
    def __init__(self, node_feat_dim, global_feat_dim, edge_feat_dim,
                 gat_out_channels_1, gat_out_channels_2, num_heads, gat_dropout,
                 fc_hidden_dim, fc_dropout):
        super().__init__()
        self.global_feat_dim = max(0, global_feat_dim)

        self.gat1 = GATv2Conv(node_feat_dim, gat_out_channels_1, heads=num_heads, dropout=gat_dropout, edge_dim=edge_feat_dim)
        self.bn1 = BatchNorm1d(gat_out_channels_1 * num_heads)
        
        self.gat2 = GATv2Conv(gat_out_channels_1 * num_heads, gat_out_channels_2, heads=num_heads, dropout=gat_dropout, edge_dim=edge_feat_dim)
        self.bn2 = BatchNorm1d(gat_out_channels_2 * num_heads)
        self.relu = ReLU()
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ íˆ¬ì˜ ë ˆì´ì–´ ì¶”ê°€
        self.res_proj1 = Linear(node_feat_dim, gat_out_channels_1 * num_heads)
        self.res_proj2 = Linear(gat_out_channels_1 * num_heads, gat_out_channels_2 * num_heads)
        
        fc1_input_dim = gat_out_channels_2 * num_heads + self.global_feat_dim
        self.fc1 = Linear(fc1_input_dim, fc_hidden_dim)
        self.dropout_fc = Dropout(fc_dropout)
        self.fc2 = Linear(fc_hidden_dim, 1)

    def forward(self, data):
        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
        
        if self.global_feat_dim > 0:
            global_feat = data.global_feat.to(x.device)
        else:
            global_feat = torch.empty((x.shape[0] if batch is None else batch.max().item() + 1, 0), device=x.device)
            
        edge_attr = edge_attr.to(x.device)

        # ì²« ë²ˆì§¸ GAT ì¸µì— ì”ì°¨ ì—°ê²° ì ìš©
        x_skip1 = x 
        x = self.relu(self.bn1(self.gat1(x_skip1, edge_index, edge_attr))) # GATv2Convì˜ ì…ë ¥ì€ x_skip1ë¡œ
        x = x + self.res_proj1(x_skip1) # íˆ¬ì˜ëœ ì…ë ¥ + ì¶œë ¥

        # ë‘ ë²ˆì§¸ GAT ì¸µì— ì”ì°¨ ì—°ê²° ì ìš©
        x_skip2 = x 
        x = self.relu(self.bn2(self.gat2(x_skip2, edge_index, edge_attr))) # GATv2Convì˜ ì…ë ¥ì€ x_skip2ë¡œ
        x = x + self.res_proj2(x_skip2) 

        x = global_mean_pool(x, batch)
        
        if self.global_feat_dim > 0:
            x = torch.cat([x, global_feat], dim=1)

        x = self.dropout_fc(self.relu(self.fc1(x)))
        return self.fc2(x)


# --- Optuna Objective í•¨ìˆ˜ ---
def objective(trial):
    # íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
    gat_out_channels_1 = trial.suggest_categorical('gat_out_channels_1', [64, 128, 256])
    gat_out_channels_2 = trial.suggest_categorical('gat_out_channels_2', [128, 256, 512])
    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])
    gat_dropout = trial.suggest_uniform('gat_dropout', 0.1, 0.4)

    fc_hidden_dim = trial.suggest_categorical('fc_hidden_dim', [64, 128, 256])
    fc_dropout = trial.suggest_uniform('fc_dropout', 0.2, 0.5)

    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)

    # K-Fold êµì°¨ ê²€ì¦ ì‹œì‘
    kf = KFold(n_splits=Config.n_splits, shuffle=True, random_state=Config.seed)
    fold_val_scores = []
    
    global_feat_dim_actual = Config.rdkit_desc_dim + Config.maccs_keys_dim + Config.use_topN

    for fold, (train_indices, val_indices) in enumerate(kf.split(train_data)):
        
        fold_train_data = [train_data[i] for i in train_indices]
        fold_val_data = [train_data[i] for i in val_indices]
        
        fold_val_y_original_indices = [d.idx_in_original_y for d in fold_val_data]
        fold_val_y_original = y_original_initial[fold_val_y_original_indices] 

        fold_train_loader = DataLoader(fold_train_data, batch_size=Config.batch_size, shuffle=True)
        fold_val_loader = DataLoader(fold_val_data, batch_size=Config.batch_size)

        model = GATv2WithGlobal(
            node_feat_dim=Config.node_feat_dim,
            global_feat_dim=global_feat_dim_actual,
            edge_feat_dim=Config.edge_feat_dim,
            gat_out_channels_1=gat_out_channels_1,
            gat_out_channels_2=gat_out_channels_2,
            num_heads=num_heads,
            gat_dropout=gat_dropout,
            fc_hidden_dim=fc_hidden_dim,
            fc_dropout=fc_dropout
        ).to(device)
        
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        loss_fn = torch.nn.SmoothL1Loss()

        best_fold_score = -np.inf
        fold_counter = 0

        for epoch_in_fold in range(1, Config.epochs + 1):
            model.train()
            for batch in fold_train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                out = model(batch)
                loss = loss_fn(out, batch.y.unsqueeze(1))
                loss.backward()
                optimizer.step()

            model.eval()
            preds_list_transformed = []
            with torch.no_grad():
                for batch in fold_val_loader:
                    batch = batch.to(device)
                    out = model(batch)
                    preds_list_transformed.append(out.cpu().numpy())

            preds_transformed = np.concatenate(preds_list_transformed).squeeze() 
            preds_original_scale = preds_transformed 

            rmse = np.sqrt(mean_squared_error(fold_val_y_original, preds_original_scale))
            corr = pearsonr(fold_val_y_original, preds_original_scale)[0]
            
            y_range = fold_val_y_original.max() - fold_val_y_original.min()
            normalized_rmse = rmse / y_range if y_range > 0 else 1.0
            score = 0.5 * (1 - min(normalized_rmse, 1)) + 0.5 * np.clip(corr, 0, 1)

            # Optuna Prunerì— ë³´ê³ 
            current_optuna_step_for_pruning = (fold * Config.epochs) + epoch_in_fold
            
            trial.report(score, current_optuna_step_for_pruning)
            if trial.should_prune():
                raise optuna.exceptions.TrialPruned()

            if score > best_fold_score:
                best_fold_score = score
                fold_counter = 0
            else:
                fold_counter += 1
                if fold_counter >= Config.patience:
                    break # Early Stopping

        fold_val_scores.append(best_fold_score)

    mean_val_score = np.mean(fold_val_scores)
    return mean_val_score

# --- Optuna Study ìƒì„± ë° ì‹¤í–‰ ---
print("\nOptuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
sampler = TPESampler(seed=Config.seed)
pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10, interval_steps=5)

# study_name ë³€ê²½: ì”ì°¨ ì—°ê²° ì¶”ê°€ë¥¼ ëª…ì‹œ
study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner, study_name="gatv2_kfold_tuning_extended_node_feat_es_final_k3_n100_lr_scheduler_res_conn") 
# â­ ì—¬ê¸°ì—ì„œ n_trials ê°’ì„ ì¡°ì •í•˜ì„¸ìš” (ì˜ˆ: 200) â­
study.optimize(objective, n_trials=300, show_progress_bar=True)

print("\n--- Optuna íŠœë‹ ê²°ê³¼ ---")
print("Best trial:")
trial = study.best_trial

print(f"   Value (Mean K-Fold Validation Score): {trial.value:.4f}")
print("   Params: ")
for key, value in trial.params.items():
    print(f"     {key}: {value}")

# â­â­â­ Optuna ì‹œê°í™” ì½”ë“œ ì‹œì‘ â­â­â­
print("\n--- Optuna ì‹œê°í™” ---")

# ìµœì í™” ì´ë ¥ ì‹œê°í™” (ì„±ëŠ¥ ê°œì„  ì¶”ì´)
fig_history = optuna.visualization.plot_optimization_history(study)
fig_history.show()
# fig_history.write_image("./_save/optuna_optimization_history.png") # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ë©´ ì´ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ì‹œê°í™”
fig_importance = optuna.visualization.plot_param_importances(study)
fig_importance.show()
# fig_importance.write_image("./_save/optuna_param_importances.png")

# ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ê°„ì˜ ê´€ê³„ ì‹œê°í™” (ìŠ¬ë¼ì´ìŠ¤ í”Œë¡¯)
fig_slice = optuna.visualization.plot_slice(study)
fig_slice.show()
# fig_slice.write_image("./_save/optuna_slice_plot.png")

# (ì„ íƒ ì‚¬í•­) í‰í–‰ ì¢Œí‘œ í”Œë¡¯: ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— ì‹œê°í™”
# fig_parallel = optuna.visualization.plot_parallel_coordinate(study)
# fig_parallel.show()
# fig_parallel.write_image("./_save/optuna_parallel_coordinate.png")
# â­â­â­ Optuna ì‹œê°í™” ì½”ë“œ ë â­â­â­

# --- ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ---
print("\nìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œì‘...")

test_loader = DataLoader(test_data, batch_size=Config.batch_size)

# ìµœì¢… í•™ìŠµì„ ìœ„í•œ í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
train_indices_for_final_split, val_indices_for_final_split = train_test_split(
    list(range(len(train_data))), # <-- ê´„í˜¸ ìˆ˜ì •ëœ ë¶€ë¶„
    test_size=0.1, # 10%ë¥¼ ìµœì¢… ê²€ì¦ ì„¸íŠ¸ë¡œ ì‚¬ìš©
    random_state=Config.seed,
    shuffle=True
)

train_data_final_split = [train_data[i] for i in train_indices_for_final_split]
val_data_final_split = [train_data[i] for i in val_indices_for_final_split]

val_y_original_indices_final = [d.idx_in_original_y for d in val_data_final_split]
val_y_original_final = y_original_initial[val_y_original_indices_final]

full_train_loader = DataLoader(train_data_final_split, batch_size=Config.batch_size, shuffle=True)
final_val_loader = DataLoader(val_data_final_split, batch_size=Config.batch_size)

final_global_feat_dim = Config.rdkit_desc_dim + Config.maccs_keys_dim + Config.use_topN
final_model = GATv2WithGlobal(
    node_feat_dim=Config.node_feat_dim,
    global_feat_dim=final_global_feat_dim,
    edge_feat_dim=Config.edge_feat_dim,
    gat_out_channels_1=trial.params['gat_out_channels_1'],
    gat_out_channels_2=trial.params['gat_out_channels_2'],
    num_heads=trial.params['num_heads'],
    gat_dropout=trial.params['gat_dropout'],
    fc_hidden_dim=trial.params['fc_hidden_dim'],
    fc_dropout=trial.params['fc_dropout']
).to(device)

final_optimizer = torch.optim.Adam(final_model.parameters(), lr=trial.params['learning_rate'])
final_loss_fn = torch.nn.SmoothL1Loss()

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜: ReduceLROnPlateau
final_scheduler = ReduceLROnPlateau(final_optimizer, mode='max', factor=0.5, patience=5, verbose=True)

# ëª¨ë¸ ì €ì¥ íŒŒì¼ëª… ë³€ê²½: ì”ì°¨ ì—°ê²° ì¶”ê°€ë¥¼ ëª…ì‹œ
final_model_save_path = './_save/final_best_gatv2_model_from_optuna_es_14feat_k3_n100_lr_scheduler_res_conn.pt' 
final_best_val_score = -np.inf
final_patience_counter = 0

print("Final model training with Early Stopping and Learning Rate Scheduler...")
for epoch in range(1, Config.epochs + 1):
    final_model.train()
    total_train_loss = 0
    for batch in tqdm(full_train_loader, desc=f"Final Train Epoch {epoch}"):
        batch = batch.to(device)
        final_optimizer.zero_grad()
        out = final_model(batch)
        loss = final_loss_fn(out, batch.y.unsqueeze(1))
        loss.backward()
        final_optimizer.step()
        total_train_loss += loss.item()
    
    avg_train_loss = total_train_loss / len(full_train_loader)

    final_model.eval()
    val_preds_list = []
    total_val_loss = 0
    with torch.no_grad():
        for batch in final_val_loader:
            batch = batch.to(device)
            out = final_model(batch)
            loss = final_loss_fn(out, batch.y.unsqueeze(1))
            total_val_loss += loss.item()
            val_preds_list.append(out.cpu().numpy())
    
    avg_val_loss = total_val_loss / len(final_val_loader)
    val_preds_original_scale = np.concatenate(val_preds_list).squeeze()

    val_rmse = np.sqrt(mean_squared_error(val_y_original_final, val_preds_original_scale))
    val_corr = pearsonr(val_y_original_final, val_preds_original_scale)[0]

    val_y_range = val_y_original_final.max() - val_y_original_final.min()
    val_normalized_rmse = val_rmse / val_y_range if val_y_range > 0 else 1.0
    current_val_score = 0.5 * (1 - min(val_normalized_rmse, 1)) + 0.5 * np.clip(val_corr, 0, 1)

    print(f"--- [Final Epoch {epoch}/{Config.epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Score: {current_val_score:.4f} ---")

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    final_scheduler.step(current_val_score)

    if current_val_score > final_best_val_score:
        final_best_val_score = current_val_score
        torch.save(final_model.state_dict(), final_model_save_path)
        print(f"âœ… ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_save_path} (Val Score: {final_best_val_score:.4f})")
        final_patience_counter = 0
    else:
        final_patience_counter += 1
        if final_patience_counter >= Config.patience:
            print(f"ğŸš« Early Stopping! Validation score did not improve for {Config.patience} epochs.")
            break

# --- í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ---
print("\n--- í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œì‘ ---")
final_model.load_state_dict(torch.load(final_model_save_path)) # ìµœì  ëª¨ë¸ ë¡œë“œ
final_model.eval()
test_preds_transformed = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc="Predicting Test Data"):
        batch = batch.to(device)
        out = final_model(batch)
        test_preds_transformed.append(out.cpu().numpy())

final_test_predictions = np.concatenate(test_preds_transformed).squeeze()

submit_df['Inhibition'] = np.nan
for i, pred_val in zip(test_submission_original_indices, final_test_predictions):
    submit_df.loc[i, 'Inhibition'] = max(0, pred_val)

submit_df['Inhibition'] = submit_df['Inhibition'].fillna(0)

# ì œì¶œ íŒŒì¼ëª… ë³€ê²½: ì”ì°¨ ì—°ê²° ì¶”ê°€ë¥¼ ëª…ì‹œ
submission_file_name = f'./_save/gnn_gatv2_optuna_kfold_submission_es_14feat_k3_n100_lr_scheduler_res_conn.csv' 
submit_df.to_csv(submission_file_name, index=False)

# --- ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---
print("\n--- [ìµœì¢… ê²°ê³¼ - Optuna íŠœë‹ ì™„ë£Œ] ---")
print(f"Optuna Best Trial Mean K-Fold Validation Score : {trial.value:.4f}")
print(f"Final Model Best Validation Score (with ES): {final_best_val_score:.4f}")
print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_file_name}")