import numpy as np
import pandas as pd
import os, platform
from tqdm import tqdm
from rdkit import Chem
from rdkit.Chem import Descriptors, MACCSkeys
from rdkit.Chem.AllChem import rdmolops
import torch
from torch.nn import Linear, Module, ReLU, Dropout, BatchNorm1d, ModuleList
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GATv2Conv, global_mean_pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import optuna # Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# ì„¤ì •
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
os.makedirs('./_save', exist_ok=True)

plt.rcParams['font.family'] = 'Malgun Gothic' if platform.system() == 'Windows' else 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train = pd.read_csv('./_data/dacon/new/train.csv').drop_duplicates('Canonical_Smiles').reset_index(drop=True)
test = pd.read_csv('./_data/dacon/new/test.csv')
submit_df_original = pd.read_csv('./_data/dacon/new/sample_submission.csv') # ìµœì¢… ì œì¶œìš© ì›ë³¸ ì €ì¥

smiles_train = train['Canonical_Smiles'].tolist()
y = train['Inhibition'].values
smiles_test = test['Canonical_Smiles'].tolist()

# SMARTS ë° ë…¸ë“œ í”¼ì²˜ ì •ì˜
smart_patterns_raw = ['[C]=[O]', '[NH2]', 'c1ccccc1']
smarts_patterns = [Chem.MolFromSmarts(p) for p in smart_patterns_raw if Chem.MolFromSmarts(p)]
electronegativity = {1: 2.20, 6: 2.55, 7: 3.04, 8: 3.44, 9: 3.98, 15: 2.19, 16: 2.58, 17: 3.16, 35: 2.96, 53: 2.66, 14: 1.90}
covalent_radius = {1: 0.31, 6: 0.76, 7: 0.71, 8: 0.66, 9: 0.57, 15: 1.07, 16: 1.05, 17: 1.02, 35: 1.20, 53: 1.39, 14: 1.11}

rdkit_desc_list = [
    Descriptors.MolWt, Descriptors.MolLogP, Descriptors.TPSA,
    Descriptors.NumRotatableBonds, Descriptors.NumHAcceptors, Descriptors.NumHDonors,
    Descriptors.HeavyAtomCount, Descriptors.FractionCSP3, Descriptors.RingCount,
    Descriptors.NHOHCount, Descriptors.NOCount, Descriptors.NumAliphaticRings,
    Descriptors.NumAromaticRings, Descriptors.NumSaturatedRings, Descriptors.ExactMolWt,
    Descriptors.MolMR, Descriptors.LabuteASA, Descriptors.BalabanJ,
    Descriptors.MaxPartialCharge, Descriptors.MinPartialCharge
]

def atom_features(atom):
    num = atom.GetAtomicNum()
    eneg = electronegativity.get(num, 0)
    radius = covalent_radius.get(num, 0)
    smarts_match = sum(atom.GetOwningMol().HasSubstructMatch(p) for p in smarts_patterns)
    return [
        num, eneg, radius,
        atom.GetTotalNumHs(), int(atom.GetHybridization()), atom.GetFormalCharge(),
        int(atom.GetIsAromatic()), int(atom.IsInRing()), smarts_match
    ]

def get_rdkit_and_maccs(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    try:
        desc = [f(mol) for f in rdkit_desc_list]
        desc = np.nan_to_num(desc).astype(np.float32)
        maccs = MACCSkeys.GenMACCSKeys(mol)
        maccs_arr = np.zeros((167,), dtype=np.int8)
        from rdkit import DataStructs
        DataStructs.ConvertToNumpyArray(maccs, maccs_arr)
        return np.concatenate([desc, maccs_arr.astype(np.float32)])
    except:
        return None

def smiles_to_graph_with_global(smiles, label=None):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None: return None
    global_feat = get_rdkit_and_maccs(smiles)
    if global_feat is None: return None
    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)
    adj = rdmolops.GetAdjacencyMatrix(mol)
    edge_index = (torch.tensor(adj) > 0).nonzero(as_tuple=False).t().contiguous()
    
    # *** ì¤‘ìš” ìˆ˜ì •: global_featë¥¼ (1, feature_dim) í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤. ***
    global_feat = torch.tensor(global_feat, dtype=torch.float).unsqueeze(0) 
    
    if label is not None:
        y = torch.tensor([label], dtype=torch.float)
        return Data(x=x, edge_index=edge_index, y=y, global_feat=global_feat)
    else:
        return Data(x=x, edge_index=edge_index, global_feat=global_feat)

# ì „ì²˜ë¦¬
data_list = [smiles_to_graph_with_global(s, l) for s, l in zip(smiles_train, y)]
data_list = [d for d in data_list if d is not None]
test_data = [smiles_to_graph_with_global(s) for s in smiles_test]
test_data = [d for d in test_data if d is not None]

# GATv2 ëª¨ë¸ í´ë˜ìŠ¤ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì •)
class GATv2WithGlobal(Module):
    def __init__(self, node_feat_dim, global_feat_dim,
                 gat_out_channels_1, num_heads_1, gat_dropout_rate,
                 gat_out_channels_2, num_heads_2,
                 mlp_hidden_dim, mlp_dropout_rate):
        super().__init__()
        self.gat1 = GATv2Conv(node_feat_dim, gat_out_channels_1, heads=num_heads_1, dropout=gat_dropout_rate)
        self.bn1 = BatchNorm1d(gat_out_channels_1 * num_heads_1)

        self.gat2 = GATv2Conv(gat_out_channels_1 * num_heads_1, gat_out_channels_2, heads=num_heads_2, dropout=gat_dropout_rate)
        self.bn2 = BatchNorm1d(gat_out_channels_2 * num_heads_2)

        self.relu = ReLU()
        self.gnn_mlp_dropout = Dropout(mlp_dropout_rate)

        # ìµœì¢… MLP ë ˆì´ì–´
        self.fc1 = Linear(gat_out_channels_2 * num_heads_2 + global_feat_dim, mlp_hidden_dim)
        self.fc2 = Linear(mlp_hidden_dim, 1)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        global_feat = data.global_feat.to(x.device) # global_featëŠ” ì´ì œ (batch_size, global_feat_dim) í˜•íƒœ

        # GATv2 ë ˆì´ì–´ ì ìš©
        x = self.relu(self.bn1(self.gat1(x, edge_index)))
        x = self.relu(self.bn2(self.gat2(x, edge_index)))

        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        x = global_mean_pool(x, batch)
        
        # *** ì¤‘ìš” ìˆ˜ì •: global_feat ì°¨ì› ì¡°ì • ë¡œì§ ì œê±° ***
        # DataLoaderê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ë°°ì¹˜ í˜•íƒœë¡œ global_featë¥¼ ìŒ“ì•„ì£¼ë¯€ë¡œ ë¶ˆí•„ìš”
        x = torch.cat([x, global_feat], dim=1)

        # MLP ë ˆì´ì–´ ì ìš©
        x = self.gnn_mlp_dropout(self.relu(self.fc1(x)))
        return self.fc2(x).squeeze()

# Optuna Objective í•¨ìˆ˜
def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
    gat_out_channels_1 = trial.suggest_int('gat_out_channels_1', 32, 128, step=32)
    num_heads_1 = trial.suggest_categorical('num_heads_1', [2, 4, 8])
    gat_out_channels_2 = trial.suggest_int('gat_out_channels_2', 64, 256, step=32)
    num_heads_2 = trial.suggest_categorical('num_heads_2', [2, 4, 8])
    gat_dropout_rate = trial.suggest_float('gat_dropout_rate', 0.1, 0.5, step=0.1)

    mlp_hidden_dim = trial.suggest_int('mlp_hidden_dim', 64, 256, step=64)
    mlp_dropout_rate = trial.suggest_float('mlp_dropout_rate', 0.1, 0.5, step=0.1)

    # suggest_loguniform ëŒ€ì‹  suggest_float(log=True) ì‚¬ìš©
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True) 
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])
    
    epochs = 100 # Optuna Trialë‹¹ ìµœëŒ€ ì—í­ ìˆ˜ (ì¡°ê¸° ì¢…ë£Œ ì ìš©)
    patience = 15 # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ patience (Optuna Trialì—ì„œëŠ” ì•½ê°„ ì§§ê²Œ)


    # ë°ì´í„° ë¡œë” ì¬ìƒì„± (Batch Size ë³€ê²½ ê°€ëŠ¥ì„± ë•Œë¬¸ì—)
    train_idx, val_idx = train_test_split(range(len(data_list)), test_size=0.2, random_state=seed)
    train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=batch_size, shuffle=True)
    val_loader = DataLoader([data_list[i] for i in val_idx], batch_size=batch_size)

    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”
    model = GATv2WithGlobal(
        node_feat_dim=9, # ê³ ì •
        global_feat_dim=187, # ê³ ì •
        gat_out_channels_1=gat_out_channels_1,
        num_heads_1=num_heads_1,
        gat_dropout_rate=gat_dropout_rate,
        gat_out_channels_2=gat_out_channels_2,
        num_heads_2=num_heads_2,
        mlp_hidden_dim=mlp_hidden_dim,
        mlp_dropout_rate=mlp_dropout_rate
    ).to(device)

    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.SmoothL1Loss()
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (ê²€ì¦ ì ìˆ˜ ê¸°ì¤€)
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6, verbose=False)

    best_score_trial = -np.inf
    counter = 0 # ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°

    for epoch in range(1, epochs + 1):
        # í•™ìŠµ
        model.train()
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            out = model(batch)
            loss = loss_fn(out, batch.y.view(-1))
            loss.backward()
            optimizer.step()

        # ê²€ì¦
        model.eval()
        preds, trues = [], []
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                out = model(batch)
                preds.append(out.cpu().numpy())
                trues.append(batch.y.view(-1).cpu().numpy())
        preds, trues = np.concatenate(preds), np.concatenate(trues)

        rmse = np.sqrt(mean_squared_error(trues, preds))
        corr = pearsonr(trues, preds)[0]
        y_range = trues.max() - trues.min()
        normalized_rmse = rmse / y_range
        score = 0.5 * (1 - min(normalized_rmse, 1)) + 0.5 * np.clip(corr, 0, 1)

        scheduler.step(score) # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸

        # Optunaì— ì ìˆ˜ ë³´ê³  ë° Pruning
        trial.report(score, epoch)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

        # ì¡°ê¸° ì¢…ë£Œ ë¡œì§ (í˜„ì¬ Trial ë‚´ì—ì„œë§Œ ì ìš©)
        if score > best_score_trial:
            best_score_trial = score
            counter = 0 # ìµœê³  ì ìˆ˜ ê°±ì‹  ì‹œ ì¹´ìš´í„° ì´ˆê¸°í™”
        else:
            counter += 1
            if counter >= patience:
                break # ì¡°ê¸° ì¢…ë£Œ

    return best_score_trial

# Optuna ìŠ¤í„°ë”” ìƒì„± ë° ìµœì í™”
print("Optuna ìµœì í™” ì‹œì‘...")
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=seed)) # seed ì¶”ê°€
study.optimize(objective, n_trials=50, timeout=3600) # ì˜ˆì‹œ: 50ë²ˆì˜ ì‹œë„ ë˜ëŠ” 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ

print("\nOptuna ìµœì í™” ì™„ë£Œ!")
print(f"ìµœì ì˜ Trial ê°’ (Best Trial Score): {study.best_trial.value:.4f}")
print(f"ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study.best_trial.params}")

# Optunaë¡œ ì°¾ì€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
print("\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘...")
best_params = study.best_trial.params

# ìµœì¢… í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¡œë” (Optuna Trialê³¼ ë™ì¼í•œ ë¶„í•  ì‚¬ìš©)
train_idx, val_idx = train_test_split(range(len(data_list)), test_size=0.2, random_state=seed)
train_loader = DataLoader([data_list[i] for i in train_idx], batch_size=best_params['batch_size'], shuffle=True)
val_loader = DataLoader([data_list[i] for i in val_idx], batch_size=best_params['batch_size'])
test_loader = DataLoader(test_data, batch_size=best_params['batch_size']) # í…ŒìŠ¤íŠ¸ ë¡œë”ë„ ìµœì  ë°°ì¹˜ í¬ê¸° ì‚¬ìš©

final_model = GATv2WithGlobal(
    node_feat_dim=9,
    global_feat_dim=187,
    gat_out_channels_1=best_params['gat_out_channels_1'],
    num_heads_1=best_params['num_heads_1'],
    gat_dropout_rate=best_params['gat_dropout_rate'],
    gat_out_channels_2=best_params['gat_out_channels_2'],
    num_heads_2=best_params['num_heads_2'],
    mlp_hidden_dim=best_params['mlp_hidden_dim'],
    mlp_dropout_rate=best_params['mlp_dropout_rate']
).to(device)

final_optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['lr'])
final_loss_fn = torch.nn.SmoothL1Loss()
final_scheduler = ReduceLROnPlateau(final_optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6, verbose=True) # verbose=Trueë¡œ ë³€ê²½

best_final_score, best_final_rmse, best_final_corr = -np.inf, 0, 0
final_counter, final_patience = 20, 20 # ìµœì¢… í•™ìŠµì—ì„œëŠ” patienceë¥¼ ì¢€ ë” ê¸¸ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŒ

for epoch in range(1, 201): # ìµœì¢… í•™ìŠµ ì—í­ì€ ë” ê¸¸ê²Œ ì„¤ì •
    final_model.train()
    for batch in train_loader:
        batch = batch.to(device)
        final_optimizer.zero_grad()
        out = final_model(batch)
        loss = final_loss_fn(out, batch.y.view(-1))
        loss.backward()
        final_optimizer.step()

    final_model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in val_loader:
            batch = batch.to(device)
            out = final_model(batch)
            preds.append(out.cpu().numpy())
            trues.append(batch.y.view(-1).cpu().numpy())
    preds, trues = np.concatenate(preds), np.concatenate(trues)

    rmse = np.sqrt(mean_squared_error(trues, preds))
    corr = pearsonr(trues, preds)[0]
    std = np.std(trues)
    normalized_rmse = rmse / std if std != 0 else 0 
    score = 0.5 * (1 - min(normalized_rmse, 1)) + 0.5 * np.clip(corr, 0, 1)

    print(f"[Final Epoch {epoch}] RMSE: {rmse:.4f}, Corr: {corr:.4f}, Score: {score:.4f}")

    final_scheduler.step(score) # ìµœì¢… ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸

    if score > best_final_score:
        best_final_score = score
        best_final_rmse = rmse
        best_final_corr = corr
        final_counter = 0
        torch.save(final_model.state_dict(), './_save/op_gnn_node_rdkit_maccs_optuna_best_model.pt')
    else:
        final_counter += 1
        if final_counter >= final_patience:
            print("Final Training EarlyStopping")
            break

# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
final_model.load_state_dict(torch.load('./_save/op_gnn_node_rdkit_maccs_optuna_best_model.pt'))
final_model.eval()
test_preds = []
with torch.no_grad():
    for batch in test_loader:
        batch = batch.to(device)
        out = final_model(batch)
        test_preds.append(out.cpu().numpy())
submit_df = submit_df_original.copy() # ì›ë³¸ ì œì¶œ ì–‘ì‹ ì‚¬ìš©
submit_df['Inhibition'] = np.concatenate(test_preds)
submit_df.to_csv('./_save/op_gnn_node_rdkit_maccs_optuna_submission.csv', index=False)

# ğŸ¯ ìµœì¢… ê²°ê³¼ ì¶œë ¥
print("\nğŸ¯ [ìµœì¢… ê²°ê³¼ - Optunaë¡œ ì°¾ì€ Best Hyperparameters ê¸°ì¤€]")
print(f"Best Score : {best_final_score:.4f}")
print(f"Best RMSE  : {best_final_rmse:.4f}")
print(f"Best Corr  : {best_final_corr:.4f}")
print("\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: ./_save/op_gnn_node_rdkit_maccs_optuna_submission.csv")