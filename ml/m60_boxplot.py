#47_00 copy
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler, MinMaxScaler
from sklearn. metrics import r2_score
import time
import random
import matplotlib.pyplot as plt

seed = 123
random.seed(seed)
np.random.seed(seed)

#1. ë°ì´í„°
datasets = fetch_california_housing()
df = pd.DataFrame(datasets.data, columns=datasets.feature_names)
df['target'] = datasets.target

df.boxplot()
# plt.show()

# ì´ìƒì¹˜ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë¹ˆ ë”•ì…”ë„ˆë¦¬
outlier_counts = {}

# IQR ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ê°œìˆ˜ ê³„ì‚°
for col in df.columns[:-1]:  # 'target' ì œì™¸
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    
    # ì´ìƒì¹˜ ê°œìˆ˜
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    outlier_counts[col] = len(outliers)

# ì´ìƒì¹˜ ê°œìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
sorted_outliers = sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True)

# ì´ìƒì¹˜ê°€ 0ê°œ ì´ˆê³¼ì¸ ì»¬ëŸ¼ë§Œ ì¶œë ¥
print("ğŸ“Œ ì´ìƒì¹˜ê°€ ë§ì€ ì»¬ëŸ¼:")
for col, count in sorted_outliers:
    if count > 0:
        print(f"{col}: {count}ê°œ")

x = datasets.data
y = datasets.target

# log ë³€í™˜í•  ì»¬ëŸ¼ ìˆ˜ë™ ì§€ì •
log_cols = ['AveBedrms', 'Population', 'AveOccup', 'MedInc', 'AveRooms']

# log1p ë³€í™˜ ì ìš©
for col in log_cols:
    df[col] = np.log1p(df[col])

log_x = np.log1p(x)
log_y = np.log1p(y)

# ê²°ê³¼ ì €ì¥ìš©
results = {}

# 1. ê¸°ë³¸ (x, y ì›ë³¸)
x1, y1 = x, y
x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.2, random_state=seed)
model = RandomForestRegressor(random_state=seed)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
results["log ë³€í™˜ ì „ score : "] = r2_score(y_test, y_pred)

# 2. yë§Œ log
x2, y2 = x, log_y
x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.2, random_state=seed)
model = RandomForestRegressor(random_state=seed)
model.fit(x_train, y_train)
y_pred_log = model.predict(x_test)
y_pred = np.expm1(y_pred_log)         # ì—­ë³€í™˜
y_true = np.expm1(y_test)             # í‰ê°€ ëŒ€ìƒë„ ì—­ë³€í™˜í•´ì•¼ í•¨
results["yë§Œ log ë³€í™˜ score : "] = r2_score(y_true, y_pred)

# 3. xë§Œ log
x3, y3 = log_x, y
x_train, x_test, y_train, y_test = train_test_split(x3, y3, test_size=0.2, random_state=seed)
model = RandomForestRegressor(random_state=seed)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
results["xë§Œ log ë³€í™˜ score : "] = r2_score(y_test, y_pred)

# 4. x, y ëª¨ë‘ log
x4, y4 = log_x, log_y
x_train, x_test, y_train, y_test = train_test_split(x4, y4, test_size=0.2, random_state=seed)
model = RandomForestRegressor(random_state=seed)
model.fit(x_train, y_train)
y_pred_log = model.predict(x_test)
y_pred = np.expm1(y_pred_log)
y_true = np.expm1(y_test)
results["x,y log ë³€í™˜ score : "] = r2_score(y_true, y_pred)

# 5. x ì¼ë¶€ë§Œ log ë³€í™˜
x_log_partial = df[datasets.feature_names].values
x5, y5 = x_log_partial, y
x_train, x_test, y_train, y_test = train_test_split(x5, y5, test_size=0.2, random_state=seed)
model = RandomForestRegressor(random_state=seed)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
results["x ì´ìƒì¹˜ ë§ì€ ì»¬ëŸ¼ë§Œ log ë³€í™˜ score : "] = r2_score(y_test, y_pred)


# ê²°ê³¼ ì¶œë ¥
for k, v in results.items():
    print(f"{k} R2 Score: {v:.4f}")

# RandomForestRegressor ëª¨ë¸ë¡œ
# log ë³€í™˜ ì „ score :  R2 Score: 0.8122
# yë§Œ log ë³€í™˜ score :  R2 Score: 0.8142
# xë§Œ log ë³€í™˜ score :  R2 Score: 0.7327
# x,y log ë³€í™˜ score :  R2 Score: 0.7364
# x ì´ìƒì¹˜ ë§ì€ ì»¬ëŸ¼ë§Œ log ë³€í™˜ score : R2 Score: 0.8123