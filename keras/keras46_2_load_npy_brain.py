# ğŸ”¹ 44_1 copy
# ì €ì¥ëœ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ npy íŒŒì¼ë¡œ ë¶ˆëŸ¬ì™€ì„œ ëª¨ë¸ í•™ìŠµ ë“±ì— ì‚¬ìš©í•  ì¤€ë¹„ë¥¼ í•¨

import numpy as np
import pandas as pd
import time

# ì¼€ë¼ìŠ¤ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ëª¨ë¸ êµ¬ì„± ê´€ë ¨ ëª¨ë“ˆ
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Activation
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

np_path = 'c:/study25/_data/_save_npy/'

x = np.load(np_path + "keras46_01_x_train.npy")  # ì´ë¯¸ì§€ ë°ì´í„° (25000, 200, 200, 3)
y = np.load(np_path + "keras46_01_y_train.npy")  # ë¼ë²¨ ë°ì´í„° (25000,)


# 2. train / val ë‚˜ëˆ„ê¸°
x_train, x_val, y_train, y_val = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y
)

# 3. ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Conv2D(32, (3,3), padding='same', input_shape=(200, 200, 1)),
    BatchNormalization(), Activation('relu'),
    MaxPooling2D(), Dropout(0.2),

    Conv2D(64, (3,3), padding='same'),
    BatchNormalization(), Activation('relu'),
    MaxPooling2D(), Dropout(0.3),

    Conv2D(128, (3,3), padding='same'),
    BatchNormalization(), Activation('relu'),
    MaxPooling2D(), Dropout(0.4),

    Flatten(),
    Dense(256, activation='relu'), Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary Classification
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

# 4. í•™ìŠµ
hist = model.fit(
    x_train, y_train,
    epochs=100,
    batch_size=64,
    validation_data=(x_val, y_val),
    callbacks=[es],
    verbose=1
)

# 5. í‰ê°€
loss, acc = model.evaluate(x_val, y_val)
print(f"val_loss: {loss:.4f}, val_acc: {acc:.4f}")
