import numpy as np                                        
import pandas as pd    
import tensorflow as tf                                   
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.utils import class_weight 
import matplotlib.pyplot as plt
import os 

#1. ë°ì´í„° ë¡œë“œ
path = './_data/kaggle/otto/'   

train_csv = pd.read_csv(path + 'train.csv', index_col=0)
test_csv = pd.read_csv(path + 'test.csv', index_col=0) 
sampleSubmission_csv = pd.read_csv(path + 'sampleSubmission.csv')

print("train_csv shape:", train_csv.shape)
print("test_csv shape:", test_csv.shape) 
print("sampleSubmission_csv shape:", sampleSubmission_csv.shape)

print("\n--- train_csv head ---")
print(train_csv.head())
print("\n--- test_csv head (feat_X ì»¬ëŸ¼ì´ ë³´ì—¬ì•¼ í•¨) ---")
print(test_csv.head()) 
print("\n--- sampleSubmission_csv head ---")
print(sampleSubmission_csv.head())


# #################### x,y ë°ì´í„° ë¶„ë¦¬ ##########################
x = train_csv.drop(['target'], axis=1) 
y = train_csv['target'] 
                                
# íƒ€ê²Ÿ ë³€ìˆ˜ Label Encoding
le = LabelEncoder()
y = le.fit_transform(y) 

# í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (validation_data ì‚¬ìš©ì„ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¦¬)
x_train, x_val, y_train, y_val = train_test_split(
    x, y,
    test_size=0.2, 
    random_state=74,
    stratify=y 
)

# test_csvì—ì„œ 'id' ì»¬ëŸ¼ ë¶„ë¦¬ (ì´ì œ ì¸ë±ìŠ¤ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤)
test_ids = test_csv.index # test_csvì˜ ì¸ë±ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
test_features = test_csv # 'id'ê°€ ì´ë¯¸ ì¸ë±ìŠ¤ì´ë¯€ë¡œ, test_csv ìì²´ê°€ íŠ¹ì„± ë°ì´í„°ì…ë‹ˆë‹¤.

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train) 
x_val_scaled = scaler.transform(x_val)         
test_features_scaled = scaler.transform(test_features)   

# ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„± (NumPy ë°°ì—´) - ëª¨ë“  íŠ¹ì„± ì‚¬ìš©
x_train_final = x_train_scaled  # ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ëœ x_trainì€ ëª¨ë“  íŠ¹ì„±ì„ í¬í•¨
x_val_final = x_val_scaled      # ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ëœ x_valì€ ëª¨ë“  íŠ¹ì„±ì„ í¬í•¨
test_final = test_features_scaled # ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ëœ test_featuresëŠ” ëª¨ë“  íŠ¹ì„±ì„ í¬í•¨

# #################### 2. ëª¨ë¸ êµ¬ì„± ##########################
# ì…ë ¥ íŠ¹ì„± ìˆ˜ê°€ 93ê°œì´ë¯€ë¡œ input_shapeë¥¼ (93,)ìœ¼ë¡œ ì„¤ì •
model = Sequential()
model.add(Dense(512, input_shape=(93,), activation='relu'))
model.add(BatchNormalization()) # Dense layer í›„ì— BatchNormalization
model.add(Dropout(0.05))   

model.add(Dense(256, activation='relu'))
model.add(BatchNormalization()) 
model.add(Dropout(0.05))   

model.add(Dense(256, activation='relu'))
model.add(BatchNormalization()) 
model.add(Dropout(0.05))   

model.add(Dense(128, activation='relu'))
model.add(BatchNormalization()) 
model.add(Dropout(0.1))   

model.add(Dense(128, activation='relu'))
model.add(BatchNormalization()) 
model.add(Dropout(0.1))   

model.add(Dense(9, activation='softmax')) # 9ê°œì˜ í´ë˜ìŠ¤ (0~8)

save_path='./_save/keras30/otto_all_features_no_lgbm/' # ì €ì¥ ê²½ë¡œ ë³€ê²½
os.makedirs(save_path, exist_ok=True) # í´ë” ì—†ìœ¼ë©´ ìƒì„±

model.save_weights(save_path+'otto_all_features_no_lgbm_weights.h5')

# #################### 3. ì»´íŒŒì¼, í›ˆë ¨ ##########################
# í•™ìŠµë¥ ì„ ë” ë‚®ê²Œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì ì¸ í•™ìŠµ ìœ ë„
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics=['accuracy']) 

# Class Weight ê³„ì‚°
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights_dict = dict(enumerate(class_weights))
print("\nComputed Class Weights:", class_weights_dict)

es = EarlyStopping(          
    monitor='val_loss',
    mode = 'min',            
    patience=50, # patienceë¥¼ 50ìœ¼ë¡œ ë” ì¦ê°€
    restore_best_weights=True,    
) 

mcp=ModelCheckpoint(
    monitor='val_loss',
    mode='auto',
    save_best_only=True,    
    filepath=save_path+'otto_all_features_no_lgbm_best.hdf5'
)

print("\n--- ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
hist = model.fit(x_train_final, y_train, epochs=500, batch_size=128, # epochsë¥¼ 500ìœ¼ë¡œ ì¦ê°€
             verbose=2, 
             validation_data=(x_val_final, y_val),
             callbacks=[es, mcp],
             class_weight=class_weights_dict 
)
print("--- ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ---")

model.save(save_path+'otto_all_features_no_lgbm_final_model.h5')

# #################### 4. í‰ê°€, ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ##########################

# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° f1 score ê³„ì‚°
val_pred_probs = model.predict(x_val_final) 
val_preds = np.argmax(val_pred_probs, axis=1) 
val_f1 = f1_score(y_val, val_preds, average='macro') 
print('\nValidation Macro F1 Score:', val_f1)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
test_pred_probs = model.predict(test_final)   

# ì œì¶œ íŒŒì¼ ìƒì„±
submission = sampleSubmission_csv.copy() 
submission['id'] = test_ids # ì €ì¥í•´ë‘” test_ids ë‹¤ì‹œ í• ë‹¹

for i in range(9):
    col_name = f'Class_{i+1}'
    submission[col_name] = test_pred_probs[:, i] 

submission.to_csv(save_path + 'sampleSubmission_all_features_no_lgbm.csv', index=False) 
print(f"\nğŸ“ '{save_path}sampleSubmission_all_features_no_lgbm.csv' ì €ì¥ ì™„ë£Œ!")

