# C:\study25ju\_data\kaggle\dog_cat\train2

import numpy as np
import pandas as pd

# ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator

# ëª¨ë¸ êµ¬ì„±ì— í•„ìš”í•œ í´ë˜ìŠ¤
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Activation

# ì •í™•ë„ ì¸¡ì •ì„ ìœ„í•œ í•¨ìˆ˜
from sklearn.metrics import accuracy_score

# ğŸ”¸ ë°ì´í„° ì „ì²˜ë¦¬: í”½ì…€ê°’ì„ 0~1ë¡œ ì •ê·œí™”
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# ğŸ”¸ ë°ì´í„°ê°€ ì €ì¥ëœ ê²½ë¡œ ì„¤ì •
path_train = './_data/kaggle/dog_cat/train2'
path_test = './_data/kaggle/dog_cat/test2'

# ğŸ”¸ í›ˆë ¨ìš© ì´ë¯¸ì§€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
xy_train = train_datagen.flow_from_directory(
    path_train,            # í›ˆë ¨ ë°ì´í„° í´ë” ê²½ë¡œ
    target_size=(70, 70),  # ì´ë¯¸ì§€ í¬ê¸° í†µì¼
    batch_size=25000,      # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    class_mode='binary',   # ì´ì§„ ë¶„ë¥˜ (ê°•ì•„ì§€ vs ê³ ì–‘ì´)
    color_mode='rgb',      # RGB ì»¬ëŸ¬ ì´ë¯¸ì§€
    shuffle=True           # ë°ì´í„°ë¥¼ ì„ì–´ì„œ ê°€ì ¸ì˜¤ê¸°
)

# ğŸ”¸ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
xy_test = test_datagen.flow_from_directory(
    path_test,
    target_size=(70, 70),
    batch_size=12500,
    class_mode='binary',
    color_mode='rgb',
    # shuffle=True            
)

# ğŸ”¸ ë°ì´í„° êµ¬ì¡° í™•ì¸
print(len(xy_train))    # í›ˆë ¨ ë°ì´í„° ë°°ì¹˜ ìˆ˜
print(len(xy_test))     # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°°ì¹˜ ìˆ˜
print(xy_train[0][0])   # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ë°°ì—´
print(xy_train[0][0].shape)  # (ì „ì²´ ê°œìˆ˜, 70, 70, 3)
print(xy_train[0][1])   # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ë¼ë²¨ (0 ë˜ëŠ” 1)
print(xy_train[0][1].shape)

# ğŸ”¸ ì‹¤ì œ ë°ì´í„° ë³€ìˆ˜ì— ì €ì¥
x_train = xy_train[0][0]
y_train = xy_train[0][1]
x_test = xy_test[0][0]
y_test = xy_test[0][1]

exit()

########################################################################
# ğŸ”¹ CNN ëª¨ë¸ êµ¬ì„±
model = Sequential()

# ğŸ”¸ Block 1
model.add(Conv2D(32, (3,3), padding='same', input_shape=(70, 70, 3))) # 32ê°œì˜ 3x3 í•„í„°
model.add(BatchNormalization())  # ì •ê·œí™”
model.add(Activation('relu'))    # ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜
model.add(MaxPooling2D())        # ë‹¤ìš´ìƒ˜í”Œë§
model.add(Dropout(0.2))          # ê³¼ì í•© ë°©ì§€

# ğŸ”¸ Block 2
model.add(Conv2D(64, (3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D())
model.add(Dropout(0.3))

# ğŸ”¸ Block 3
model.add(Conv2D(128, (3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D())
model.add(Dropout(0.4))

# ğŸ”¸ ì™„ì „ ì—°ê²°ì¸µ (FC Layer)
model.add(Flatten())                    # 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°
model.add(Dense(256, activation='relu')) # ì€ë‹‰ì¸µ
model.add(Dropout(0.5))                 # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ

# ğŸ”¸ ì¶œë ¥ì¸µ
model.add(Dense(1, activation='sigmoid')) # ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ sigmoid ì‚¬ìš©

########################################################################
# ğŸ”¹ ëª¨ë¸ ì»´íŒŒì¼
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

# ğŸ”¹ ì¡°ê¸° ì¢…ë£Œ ì½œë°±
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es = EarlyStopping(monitor='val_loss', mode='min', patience=30, restore_best_weights=True, verbose=1)

# ğŸ”¹ ëª¨ë¸ í•™ìŠµ
hist = model.fit(
    x_train, y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,  # 20%ëŠ” ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
    callbacks=[es],
    verbose=1
)

# ğŸ”¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
loss, acc = model.evaluate(x_test, y_test)
print("loss:", loss)
print("acc:", acc)

###############################################
# ğŸ”¹ ì˜ˆì¸¡ê°’ ìƒì„± ë° ì´ì§„í™”
y_pred_prob = model.predict(x_test)
y_pred = (y_pred_prob > 0.5).astype(int)  # í™•ë¥  -> 0 or 1

# ğŸ”¹ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ğŸ”¹ ì´ë¯¸ì§€ ì‹œê°í™”
aaa = 1
import matplotlib.pyplot as plt
plt.imshow(x_train[aaa], 'gray')
plt.show()

###############################################
# ğŸ”¹ í•™ìŠµ ê³¼ì • ì‹œê°í™” (Loss & Accuracy ê·¸ë˜í”„)

plt.figure(figsize=(12, 5))

# ğŸ”¸ Loss ê·¸ë˜í”„
plt.subplot(1, 2, 1)
plt.plot(hist.history['loss'], label='Train Loss')
plt.plot(hist.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

# ğŸ”¸ Accuracy ê·¸ë˜í”„
plt.subplot(1, 2, 2)
plt.plot(hist.history['acc'], label='Train Acc')
plt.plot(hist.history['val_acc'], label='Val Acc')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

#### í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(4, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')  # ì˜ˆì¸¡ í´ë˜ìŠ¤
plt.ylabel('Actual')     # ì‹¤ì œ í´ë˜ìŠ¤
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred))

#### ROC Curve ì‹œê°í™” (ì´ì§„ ë¶„ë¥˜ì—ì„œ ì„±ëŠ¥ í™•ì¸)

from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')  # ê¸°ì¤€ì„ 
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.grid()
plt.show()
